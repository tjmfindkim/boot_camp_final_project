{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensor flow for machine learning\n",
    "import tensorflow as tf\n",
    "import transfer_tools as tt\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from tensorflow.python.keras.preprocessing import image as kp_image\n",
    "from tensorflow.python.keras import model, models, losses, layers\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.models import model_from_json\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "class StyleTransferModel:\n",
    "    \n",
    "    ###########################################################################\n",
    "\n",
    "    def __init__(self):\n",
    "        # Content layer where will pull our feature maps\n",
    "        content_layers = ['block5_conv2'] \n",
    "\n",
    "        # Style layer we are interested in\n",
    "        style_layers = ['block1_conv1',\n",
    "                        'block2_conv1',\n",
    "                        'block3_conv1', \n",
    "                        'block4_conv1', \n",
    "                        'block5_conv1'\n",
    "                       ]\n",
    "        json_file = open(\"model.json\", 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        vgg = model_from_json(loaded_model_json)\n",
    "        vgg.load_weights(\"model.h5\")\n",
    "        style_outputs = [vgg.get_layer(name).output for name in style_layers]\n",
    "        content_outputs = [vgg.get_layer(name).output for name in content_layers]\n",
    "        model_outputs = style_outputs + content_outputs\n",
    "        self.model = models.Model(vgg.input, model_outputs)\n",
    "        \n",
    "        \"\"\" Creates our model with access to intermediate layers. \n",
    "\n",
    "        This function will load the VGG19 model and access the intermediate layers. \n",
    "        These layers will then be used to create a new model that will take input image\n",
    "        and return the outputs from these intermediate layers from the VGG model. \n",
    "\n",
    "        self.model is a keras model that takes image inputs and outputs the style and \n",
    "          content intermediate layers. \n",
    "        \"\"\"\n",
    "  \n",
    "    def compute_loss(self, loss_weights, init_image, gram_style_features, content_features):\n",
    "        \"\"\"This function will compute the loss total loss.\n",
    "\n",
    "        Arguments:\n",
    "        model: The model that will give us access to the intermediate layers\n",
    "        loss_weights: The weights of each contribution of each loss function. \n",
    "          (style weight, content weight, and total variation weight)\n",
    "        init_image: Our initial base image. This image is what we are updating with \n",
    "          our optimization process. We apply the gradients wrt the loss we are \n",
    "          calculating to this image.\n",
    "        gram_style_features: Precomputed gram matrices corresponding to the \n",
    "          defined style layers of interest.\n",
    "        content_features: Precomputed outputs from defined content layers of \n",
    "          interest.\n",
    "\n",
    "        Returns:\n",
    "        returns the total loss, style loss, content loss, and total variational loss\n",
    "        \"\"\"\n",
    "        style_weight, content_weight = loss_weights\n",
    "\n",
    "        # Feed our init image through our model. This will give us the content and \n",
    "        # style representations at our desired layers. Since we're using eager\n",
    "        # our model is callable just like any other function!\n",
    "        model_outputs = self.model(init_image)\n",
    "\n",
    "        style_output_features = model_outputs[:num_style_layers]\n",
    "        content_output_features = model_outputs[num_style_layers:]\n",
    "\n",
    "        style_score = 0\n",
    "        content_score = 0\n",
    "\n",
    "        # Accumulate style losses from all layers\n",
    "        # Here, we equally weight each contribution of each loss layer\n",
    "        weight_per_style_layer = 1.0 / float(num_style_layers)\n",
    "        for target_style, comb_style in zip(gram_style_features, style_output_features):\n",
    "            style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)\n",
    "\n",
    "        # Accumulate content losses from all layers \n",
    "        weight_per_content_layer = 1.0 / float(num_content_layers)\n",
    "        for target_content, comb_content in zip(content_features, content_output_features):\n",
    "            content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)\n",
    "\n",
    "        style_score *= style_weight\n",
    "        content_score *= content_weight\n",
    "\n",
    "        # Get total loss\n",
    "        loss = style_score + content_score \n",
    "        return loss, style_score, content_score\n",
    "\n",
    "\n",
    "    def get_feature_representations(self, content_path, style_path):\n",
    "        \"\"\"Helper function to compute our content and style feature representations.\n",
    "\n",
    "        This function will simply load and preprocess both the content and style \n",
    "        images from their path. Then it will feed them through the network to obtain\n",
    "        the outputs of the intermediate layers. \n",
    "\n",
    "        Arguments:\n",
    "        model: The model that we are using.\n",
    "        content_path: The path to the content image.\n",
    "        style_path: The path to the style image\n",
    "\n",
    "        Returns:\n",
    "        returns the style features and the content features. \n",
    "        \"\"\"\n",
    "        # Load our images in \n",
    "        content_image = load_and_process_img(content_path)\n",
    "        style_image = load_and_process_img(style_path)\n",
    "\n",
    "        # batch compute content and style features\n",
    "        style_outputs = self.model(style_image)\n",
    "        content_outputs = self.model(content_image)\n",
    "\n",
    "\n",
    "        # Get the style and content feature representations from our model  \n",
    "        style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]\n",
    "        content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]\n",
    "        return style_features, content_features\n",
    "\n",
    " \n",
    "    def run_style_transfer(self, content_path, \n",
    "                           style_path,\n",
    "                           num_iterations=1000,\n",
    "                           content_weight=1e3, \n",
    "                           style_weight=1e-2): \n",
    "        # We don't need to (or want to) train any layers of our model, so we set their\n",
    "        # trainable to false. \n",
    "        for layer in self.model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        # Get the style and content feature representations (from our specified intermediate layers) \n",
    "        style_features, content_features = get_feature_representations(self.model, content_path, style_path)\n",
    "        gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
    "\n",
    "        # Set initial image\n",
    "        init_image = load_and_process_img(self, content_path)\n",
    "        init_image = tfe.Variable(init_image, dtype=tf.float32)\n",
    "        # Create our optimizer\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=5, beta1=0.99, epsilon=1e-1)\n",
    "\n",
    "        # For displaying intermediate images \n",
    "        iter_count = 1\n",
    "\n",
    "        # Store our best result\n",
    "        best_loss, best_img = float('inf'), None\n",
    "\n",
    "        # Create a nice config \n",
    "        loss_weights = (style_weight, content_weight)\n",
    "        cfg = {\n",
    "          'model': self.model,\n",
    "          'loss_weights': loss_weights,\n",
    "          'init_image': init_image,\n",
    "          'gram_style_features': gram_style_features,\n",
    "          'content_features': content_features\n",
    "        }\n",
    "\n",
    "        # For displaying\n",
    "        num_rows = 2\n",
    "        num_cols = 5\n",
    "        display_interval = num_iterations/(num_rows*num_cols)\n",
    "        start_time = time.time()\n",
    "        global_start = time.time()\n",
    "\n",
    "        norm_means = np.array([103.939, 116.779, 123.68])\n",
    "        min_vals = -norm_means\n",
    "        max_vals = 255 - norm_means   \n",
    "\n",
    "        imgs = []\n",
    "        for i in range(num_iterations):\n",
    "            grads, all_loss = compute_grads(cfg)\n",
    "            loss, style_score, content_score = all_loss\n",
    "            opt.apply_gradients([(grads, init_image)])\n",
    "            clipped = tf.clip_by_value(init_image, min_vals, max_vals)\n",
    "            init_image.assign(clipped)\n",
    "            end_time = time.time() \n",
    "\n",
    "            if loss < best_loss:\n",
    "                # Update best loss and best image from total loss. \n",
    "                best_loss = loss\n",
    "                best_img = deprocess_img(init_image.numpy())\n",
    "\n",
    "            if i % display_interval== 0:\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Use the .numpy() method to get the concrete numpy array\n",
    "                plot_img = init_image.numpy()\n",
    "                plot_img = deprocess_img(plot_img)\n",
    "                imgs.append(plot_img)\n",
    "                IPython.display.clear_output(wait=True)\n",
    "                IPython.display.display_png(Image.fromarray(plot_img))\n",
    "                print('Iteration: {}'.format(i))        \n",
    "                print('Total loss: {:.4e}, ' \n",
    "                    'style loss: {:.4e}, '\n",
    "                    'content loss: {:.4e}, '\n",
    "                    'time: {:.4f}s'.format(loss, style_score, content_score, time.time() - start_time))\n",
    "        print('Total time: {:.4f}s'.format(time.time() - global_start))\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(14,4))\n",
    "        for i,img in enumerate(imgs):\n",
    "            plt.subplot(num_rows,num_cols,i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "\n",
    "        return best_img, best_loss \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
